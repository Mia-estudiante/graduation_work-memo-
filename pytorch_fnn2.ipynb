{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch_fnn2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNYqOLMz9YnfocwsD2L4Hl8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cHNeYfj4sUKm","executionInfo":{"status":"ok","timestamp":1651229380550,"user_tz":-540,"elapsed":5452,"user":{"displayName":"빵수니","userId":"07272002582005504067"}},"outputId":"1155be69-c059-43f2-f783-9dfcc561c1ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"]},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader"],"metadata":{"id":"rkeP359NsXmY","executionInfo":{"status":"ok","timestamp":1651229455969,"user_tz":-540,"elapsed":404,"user":{"displayName":"빵수니","userId":"07272002582005504067"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["#enc_output = 32 x 60 x 768 형태의 tensor 데이터\n","#label = 32 x 1 x 1 형태의 tensor 데이터 "],"metadata":{"id":"9s4cCORy8zII"}},{"cell_type":"code","source":["#768차원의 cls 토큰을 6개 라벨로 분류한 후\n","#그 중 제일 큰 값을 가지는 라벨로 매핑\n","#ex) cls 토큰(768차원) --- 감성 분류 후 --> label1(1차원) ====> 768차원으로 늘려주기 \n","class NNModel(nn.Module):\n","    def __init__(self, input_dim, class_size):\n","        super(NNModel,self).__init__()\n","        self.l1 = nn.Linear(input_dim,520) #768차원의 cls token을..\n","        self.l2 = nn.Linear(520,320)\n","        self.l3 = nn.Linear(320,240)\n","        self.l4 = nn.Linear(240,120)\n","        self.l5 = nn.Linear(120, class_size)   #6개로 classification하기 위해..\n","        \n","    def forward(self, x):\n","        x = F.relu(self.l1(x))\n","        x = F.relu(self.l2(x))\n","        x = F.relu(self.l3(x))\n","        x = F.relu(self.l4(x))\n","        return self.l5(x)"],"metadata":{"id":"-QFt7FlQFOQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################변수 선언#########################\n","\n","batch_size = 32\n","# total_batch_num = len(총 챗봇 데이터 개수) / batch_size\n","train_batch_size = 4 #32개를 train하기 위해 임의로 설정\n","input_dim = 768\n","class_size = 6\n","epochs = 10000\n","lr = 0.01\n","\n","#######################변수 선언#########################\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = NNModel(input_dim, class_size).to(device)\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=lr)\n","\n","for big_batch_num in total_batch_num:\n","  # enc_outputs(tensor) 선언 - encoder에서 output을 받아..\n","  # labels(tensor) 선언 - 해당 시퀀스의 label을 받아..\n","  train_dataset = TensorDataset(enc_outputs, labels)\n","  train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n","\n","  for epoch in range(epochs+1):\n","    cost = 0.0\n","    for (step, batch) in enumerate(train_dataloader):\n","      model.train()\n","      batch = tuple(t.cuda() for t in batch)\n","      optimizer.zero_grad()\n","\n","      enc_output, label = batch\n","      enc_output = enc_output.view(-1, input_dim).to(device)\n","      label = label.to(device)\n","\n","      hypothesis = model(enc_output)\n","      loss = criterion(hypothesis, label)\n","      loss.backward()\n","      optimizer.step()\n","\n","      cost += loss\n","  if (epoch+1)%100==0:\n","      print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"],"metadata":{"id":"dkbfT6sQsWRD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"XbaIsaZT9ZpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#######################변수 선언#########################\n","\n","sentiment_labels = {0: \"label0\", 1: \"label1\", 2: \"label2\", 3: \"label3\", 4: \"label4\", 5: \"label5\"}\n","# test_data(tensor) 선언 - enc_outputs 중 랜덤으로 가져와..\n","# labels(tensor) 선언 - 해당 test 데이터의 label을 가져와..\n","test_batch_size = 4\n","\n","#######################변수 선언#########################\n","\n","with torch.no_grad():\n","    model.eval()\n","    \n","    test_dataset = TensorDataset(test_data, labels)\n","    test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=test_batch_size)\n","\n","    for (step, batch) in enumerate(test_dataloader):\n","      batch = tuple(t.cuda() for t in batch)\n","      enc_output, label = batch\n","\n","      enc_output = enc_output.view(-1, input_dim).to(device)\n","      label = label.to(device)\n","\n","      prediction = model(enc_output)\n","\n","      print('--------------------------------')\n","      print(prediction)\n","      print(torch.round(F.softmax(prediction, dim=1), decimals=2))\n","      print(prediction.argmax(1))\n","      print(list(map(sentiment_labels.get, prediction.argmax(1).tolist())))"],"metadata":{"id":"w9cMnWSfEYpp"},"execution_count":null,"outputs":[]}]}